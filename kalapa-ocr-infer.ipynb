{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05e5c481",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-12-08T00:18:05.291522Z",
     "iopub.status.busy": "2023-12-08T00:18:05.290694Z",
     "iopub.status.idle": "2023-12-08T00:18:11.568593Z",
     "shell.execute_reply": "2023-12-08T00:18:11.567720Z"
    },
    "papermill": {
     "duration": 6.286846,
     "end_time": "2023-12-08T00:18:11.571196",
     "exception": false,
     "start_time": "2023-12-08T00:18:05.284350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a081ff7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-08T00:18:11.581466Z",
     "iopub.status.busy": "2023-12-08T00:18:11.580919Z",
     "iopub.status.idle": "2023-12-08T00:18:11.629665Z",
     "shell.execute_reply": "2023-12-08T00:18:11.628510Z"
    },
    "papermill": {
     "duration": 0.056933,
     "end_time": "2023-12-08T00:18:11.632411",
     "exception": false,
     "start_time": "2023-12-08T00:18:11.575478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Timm(nn.Module):\n",
    "    def __init__(self, name, hidden = 256, drop_path_rate=0.1, drop_rate=0.1, dropout=0.5):\n",
    "        super(Timm, self).__init__()\n",
    "\n",
    "        base_model = timm.create_model(name, pretrained=False, \n",
    "            drop_rate = drop_rate,\n",
    "            drop_path_rate =drop_path_rate\n",
    "            )\n",
    "\n",
    "        layers = list(base_model.children())[:-2]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "        in_features = base_model.num_features\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.last_conv_1x1 = nn.Conv2d(in_features, hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Shape: \n",
    "            - x: (N, C, H, W)\n",
    "            - output: (W, N, C)\n",
    "        \"\"\"\n",
    "        conv = self.encoder(x)\n",
    "\n",
    "        # conv = self.features(x)\n",
    "        conv = self.dropout(conv)\n",
    "        conv = self.last_conv_1x1(conv)\n",
    "\n",
    "        # print(conv.shape) #BxChx4x64\n",
    "\n",
    "        conv = conv.transpose(-1, -2)\n",
    "        conv = conv.flatten(2)\n",
    "\n",
    "        conv = conv.permute(-1, 0, 1)\n",
    "\n",
    "\n",
    "        return conv\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        # self.rnn1 = nn.GRU(enc_hid_dim*2, enc_hid_dim, bidirectional = True)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        src: src_len x batch_size x img_channel\n",
    "        outputs: src_len x batch_size x hid_dim \n",
    "        hidden: batch_size x hid_dim\n",
    "        \"\"\"\n",
    "\n",
    "        embedded = self.dropout(src)\n",
    "        \n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        # outputs, hidden = self.rnn1(outputs)\n",
    "                                 \n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        \n",
    "        return outputs, hidden\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        hidden: batch_size x hid_dim\n",
    "        encoder_outputs: src_len x batch_size x hid_dim,\n",
    "        outputs: batch_size x src_len\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "  \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        \n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        return F.softmax(attention, dim = 1)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        # self.rnn1 = nn.GRU(dec_hid_dim * 2, dec_hid_dim)\n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        inputs: batch_size\n",
    "        hidden: batch_size x hid_dim\n",
    "        encoder_outputs: src_len x batch_size x hid_dim\n",
    "        \"\"\"\n",
    "             \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "                \n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        # output, hidden = self.rnn1(output, hidden.unsqueeze(0))\n",
    "        \n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        \n",
    "        return prediction, hidden.squeeze(0), a.squeeze(1)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, encoder_hidden, decoder_hidden, img_channel, decoder_embedded, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        attn = Attention(encoder_hidden, decoder_hidden)\n",
    "        \n",
    "        self.encoder = Encoder(img_channel, encoder_hidden, decoder_hidden, dropout)\n",
    "        self.decoder = Decoder(vocab_size, decoder_embedded, encoder_hidden, decoder_hidden, dropout, attn)\n",
    "        \n",
    "    def forward_encoder(self, src):       \n",
    "        \"\"\"\n",
    "        src: timestep x batch_size x channel\n",
    "        hidden: batch_size x hid_dim\n",
    "        encoder_outputs: src_len x batch_size x hid_dim\n",
    "        \"\"\"\n",
    "\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "\n",
    "        return (hidden, encoder_outputs)\n",
    "\n",
    "    def forward_decoder(self, tgt, memory):\n",
    "        \"\"\"\n",
    "        tgt: timestep x batch_size \n",
    "        hidden: batch_size x hid_dim\n",
    "        encouder: src_len x batch_size x hid_dim\n",
    "        output: batch_size x 1 x vocab_size\n",
    "        \"\"\"\n",
    "        \n",
    "        tgt = tgt[-1]\n",
    "        hidden, encoder_outputs = memory\n",
    "        output, hidden, _ = self.decoder(tgt, hidden, encoder_outputs)\n",
    "        output = output.unsqueeze(1)\n",
    "        \n",
    "        return output, (hidden, encoder_outputs)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "        src: time_step x batch_size\n",
    "        trg: time_step x batch_size\n",
    "        outputs: batch_size x time_step x vocab_size\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        device = src.device\n",
    "\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "                \n",
    "        ##TODO reverse the order>> decode backward??\n",
    "        for t in range(trg_len):\n",
    "            input = trg[t] \n",
    "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs)\n",
    "            \n",
    "            outputs[t] = output\n",
    "            \n",
    "        outputs = outputs.transpose(0, 1).contiguous()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def expand_memory(self, memory, beam_size):\n",
    "        hidden, encoder_outputs = memory\n",
    "        hidden = hidden.repeat(beam_size, 1)\n",
    "        encoder_outputs = encoder_outputs.repeat(1, beam_size, 1)\n",
    "\n",
    "        return (hidden, encoder_outputs)\n",
    "    \n",
    "    def get_memory(self, memory, i):\n",
    "        hidden, encoder_outputs = memory\n",
    "        hidden = hidden[[i]]\n",
    "        encoder_outputs = encoder_outputs[:, [i],:]\n",
    "\n",
    "        return (hidden, encoder_outputs)\n",
    "\n",
    "class Vocab():\n",
    "    def __init__(self, chars):\n",
    "        self.pad = 0\n",
    "        self.go = 1\n",
    "        self.eos = 2\n",
    "        self.mask_token = 3\n",
    "\n",
    "        self.chars = chars\n",
    "\n",
    "        self.c2i = {c:i+4 for i, c in enumerate(chars)}\n",
    "\n",
    "        self.i2c = {i+4:c for i, c in enumerate(chars)}\n",
    "        \n",
    "        self.i2c[0] = '<pad>'\n",
    "        self.i2c[1] = '<sos>'\n",
    "        self.i2c[2] = '<eos>'\n",
    "        self.i2c[3] = '*'\n",
    "\n",
    "    def encode(self, chars):\n",
    "        return [self.go] + [self.c2i[c] for c in chars if c in self.c2i] + [self.eos]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        first = 1 if self.go in ids else 0\n",
    "        last = ids.index(self.eos) if self.eos in ids else None\n",
    "        sent = ''.join([self.i2c[i] for i in ids[first:last]])\n",
    "        return sent\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.c2i) + 4\n",
    "    \n",
    "    def batch_decode(self, arr):\n",
    "        texts = [self.decode(ids) for ids in arr]\n",
    "        return texts\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.chars\n",
    "\n",
    "class VietOCR(nn.Module):\n",
    "    def __init__(self, vocab, hidden_dim, backbone):\n",
    "        \n",
    "        super(VietOCR, self).__init__()\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.cnn = Timm(backbone, hidden = hidden_dim, drop_path_rate=0.5, drop_rate=0.5, dropout=0.5)\n",
    "\n",
    "        self.transformer = Seq2Seq(len(self.vocab), encoder_hidden=hidden_dim, decoder_hidden=hidden_dim, img_channel=hidden_dim, decoder_embedded=hidden_dim, dropout=0.5) \n",
    "\n",
    "    def forward(self, img, tgt_input, tgt_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Shape:\n",
    "            - img: (N, C, H, W)\n",
    "            - tgt_input: (T, N)\n",
    "            - tgt_key_padding_mask: (N, T)\n",
    "            - output: b t v\n",
    "        \"\"\"\n",
    "        src = self.cnn(img)\n",
    "        # print(src.shape)\n",
    "        outputs = self.transformer(src, tgt_input)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6fb2407",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-08T00:18:11.641917Z",
     "iopub.status.busy": "2023-12-08T00:18:11.641511Z",
     "iopub.status.idle": "2023-12-08T00:18:11.647432Z",
     "shell.execute_reply": "2023-12-08T00:18:11.646293Z"
    },
    "papermill": {
     "duration": 0.013269,
     "end_time": "2023-12-08T00:18:11.649721",
     "exception": false,
     "start_time": "2023-12-08T00:18:11.636452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vocab =  'aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~ '\n",
    "vocab = Vocab(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc16e84e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-08T00:18:11.658961Z",
     "iopub.status.busy": "2023-12-08T00:18:11.658607Z",
     "iopub.status.idle": "2023-12-08T00:18:12.915345Z",
     "shell.execute_reply": "2023-12-08T00:18:12.914135Z"
    },
    "papermill": {
     "duration": 1.265577,
     "end_time": "2023-12-08T00:18:12.919157",
     "exception": false,
     "start_time": "2023-12-08T00:18:11.653580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded!!\n"
     ]
    }
   ],
   "source": [
    "##load model\n",
    "model_b1 = VietOCR(vocab, hidden_dim = 384, backbone = 'tf_efficientnetv2_b1.in1k')\n",
    "model_b1.to(device)\n",
    "checkpoint = torch.load('/kaggle/input/ocr-weight/b1_384_f5_all_swa.pth', map_location=\"cpu\")\n",
    "model_b1.load_state_dict(checkpoint)\n",
    "model_b1.eval()\n",
    "\n",
    "model_b2 = VietOCR(vocab, hidden_dim = 256, backbone = 'tf_efficientnetv2_b2.in1k')\n",
    "model_b2.to(device)\n",
    "checkpoint = torch.load('/kaggle/input/ocr-weight/b2_256_f5_all_swa.pth', map_location=\"cpu\")\n",
    "model_b2.load_state_dict(checkpoint)\n",
    "model_b2.eval()\n",
    "print('model loaded!!')\n",
    "##~load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bda7dfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-08T00:18:12.928563Z",
     "iopub.status.busy": "2023-12-08T00:18:12.928178Z",
     "iopub.status.idle": "2023-12-08T00:18:12.934354Z",
     "shell.execute_reply": "2023-12-08T00:18:12.933264Z"
    },
    "papermill": {
     "duration": 0.013452,
     "end_time": "2023-12-08T00:18:12.936610",
     "exception": false,
     "start_time": "2023-12-08T00:18:12.923158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(image, im_w, im_h, device):\n",
    "    img_b1 = cv2.resize(image.copy(), (im_w, im_h))\n",
    "    img_b1 = img_b1.transpose(2,0,1)\n",
    "    img_b1 = img_b1/255 \n",
    "    img_b1 = torch.from_numpy(img_b1).unsqueeze(0).float().to(device)\n",
    "    return img_b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27c144e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-08T00:18:12.945967Z",
     "iopub.status.busy": "2023-12-08T00:18:12.945628Z",
     "iopub.status.idle": "2023-12-08T00:18:12.962401Z",
     "shell.execute_reply": "2023-12-08T00:18:12.961647Z"
    },
    "papermill": {
     "duration": 0.024157,
     "end_time": "2023-12-08T00:18:12.964594",
     "exception": false,
     "start_time": "2023-12-08T00:18:12.940437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(image_folder, output_file_path):\n",
    "    prediction = pd.DataFrame(columns=['id', 'answer', 'elapsed_time'])    \n",
    "    index = 0\n",
    "    for person_id in os.listdir(image_folder):\n",
    "        for image_id in os.listdir(os.path.join(image_folder, person_id)):\n",
    "            fp = os.path.join(image_folder, person_id, image_id)\n",
    "            image_id = os.path.join(person_id, image_id)\n",
    "            image = cv2.imread(fp)\n",
    "\n",
    "            # Start inference\n",
    "            start = time.time()\n",
    "            \n",
    "            img_b1 = preprocess(image, 1664, 160, device)\n",
    "            img_b2 = preprocess(image, 2048, 128, device)\n",
    "            \n",
    "            max_seq_length=128\n",
    "            sos_token=1\n",
    "            eos_token=2\n",
    "            with torch.no_grad():\n",
    "                src1 = model_b1.cnn(img_b1)\n",
    "                src2 = model_b2.cnn(img_b2)\n",
    "                \n",
    "                memory1 = model_b1.transformer.forward_encoder(src1)\n",
    "                memory2 = model_b2.transformer.forward_encoder(src2)\n",
    "                \n",
    "                translated_sentence = [[sos_token]*len(img_b1)]\n",
    "                char_probs = [[1]*len(img_b1)]\n",
    "                max_length = 0\n",
    "                while max_length <= max_seq_length and not all(np.any(np.asarray(translated_sentence).T==eos_token, axis=1)):\n",
    "\n",
    "                    tgt_inp = torch.LongTensor(translated_sentence).to(device)\n",
    "                    \n",
    "                    output1, memory1 = model_b1.transformer.forward_decoder(tgt_inp, memory1)\n",
    "                    output1 = torch.nn.functional.softmax(output1, dim=-1)\n",
    "                    output1 = output1.to('cpu')\n",
    "                    \n",
    "                    output2, memory2 = model_b2.transformer.forward_decoder(tgt_inp, memory2)\n",
    "                    output2 = torch.nn.functional.softmax(output2, dim=-1)\n",
    "                    output2 = output2.to('cpu')\n",
    "                    \n",
    "                    output = 0.5*output1 + 0.5*output2\n",
    "                    \n",
    "                    values, indices  = torch.topk(output, 5)\n",
    "                    \n",
    "                    indices = indices[:, -1, 0]\n",
    "                    indices = indices.tolist()\n",
    "                    \n",
    "                    values = values[:, -1, 0]\n",
    "                    values = values.tolist()\n",
    "                    char_probs.append(values)\n",
    "\n",
    "                    translated_sentence.append(indices)   \n",
    "                    max_length += 1\n",
    "\n",
    "                    del output\n",
    "                    \n",
    "                translated_sentence = np.asarray(translated_sentence).T\n",
    "                char_probs = np.asarray(char_probs).T\n",
    "                char_probs = np.multiply(char_probs, translated_sentence>3)\n",
    "                char_probs = np.sum(char_probs, axis=-1)/(char_probs>0).sum(-1)\n",
    "                \n",
    "                s = translated_sentence[0].tolist()\n",
    "                answer = vocab.decode(s)\n",
    "                \n",
    "            end = time.time()\n",
    "            prediction.loc[index] = [image_id, answer, end - start]\n",
    "            index += 1\n",
    "    \n",
    "    # Write prediction\n",
    "    prediction.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90ae2b8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-08T00:18:12.974329Z",
     "iopub.status.busy": "2023-12-08T00:18:12.973574Z",
     "iopub.status.idle": "2023-12-08T00:18:12.977288Z",
     "shell.execute_reply": "2023-12-08T00:18:12.976580Z"
    },
    "papermill": {
     "duration": 0.010707,
     "end_time": "2023-12-08T00:18:12.979261",
     "exception": false,
     "start_time": "2023-12-08T00:18:12.968554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#predict(image_folder='/kaggle/input/ocr-weight/test_images/test_images', output_file_path='sub.csv')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3994046,
     "sourceId": 7150385,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30579,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12.540728,
   "end_time": "2023-12-08T00:18:14.204840",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-08T00:18:01.664112",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
